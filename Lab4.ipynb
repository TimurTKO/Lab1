{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmj9tL8wQWbb0goTKBtHlJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimurTKO/Lab1/blob/main/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OvUQ2nZ2viQ",
        "outputId": "67e6c3ae-8926-479c-846d-895abe906226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2CIkrWiOxVF",
        "outputId": "f50eb540-ce0c-46db-84aa-1f2ddc650162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sql"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTkycjbPYZoW",
        "outputId": "bcf1ff09-38e8-4034-984b-4d617ffcdf3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sql\n",
            "  Downloading sql-2022.4.0.tar.gz (4.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sql\n",
            "  Building wheel for sql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sql: filename=sql-2022.4.0-py3-none-any.whl size=4307 sha256=f5e63bccbbf759f6ad879f87c50163967d46df5da93d2c7542222dba316cdda4\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/0c/6c/d177cf58d1d794fb4cd8d9d07ba89451040e7d6d1c1e01b9d9\n",
            "Successfully built sql\n",
            "Installing collected packages: sql\n",
            "Successfully installed sql-2022.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts_DeIuPPWoJ",
        "outputId": "55ad4e5e-8151-48f6-b411-99ba205ea9be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=aede6b94e29ab4149e6092c80f7d925234c3ed03596ee2e46f4102ab2b16fd9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
        "import sklearn\n",
        "sklearn.set_config(transform_output=\"pandas\")\n",
        "from sklearn.utils import compute_sample_weight\n",
        "#\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from joblib import dump, load\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from xgboost import plot_importance\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from time import sleep\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#import optuna\n",
        "\n",
        "from sklearn.metrics import fbeta_score, make_scorer\n",
        "\n",
        "#import mlflow\n"
      ],
      "metadata": {
        "id": "fm9VuhG2PcJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from pyspark import SparkContext # from pyspark import SparkContext # отвечает за операции с с кластером в Spark"
      ],
      "metadata": {
        "id": "QvS-PNgWPh-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(appName=\"IntroToSpark\") # Инициализируем объект SparkContext и передадим ему настройки.\n",
        "#Это могут быть URL-адрес мастер-узла и название приложения.\n",
        "# Например, укажем наше приложение IntroToSpark (англ. «Введение в Spark»).\n",
        "weather_entry = sc.parallelize(['2021-04-07', 17.1, 22.3]).take(3) # Вызовом функции sc.parallelize() (англ. «параллелизовать») можно перевести список в RDD.\n",
        "# А содержимое таблицы выведем методом take (взять)\n",
        "print(weather_entry)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRPeROG9Pr4U",
        "outputId": "3dca8169-c36a-4706-e35b-a9856be664e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2021-04-07', 17.1, 22.3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "APP_NAME = \"DataFrames\"\n",
        "SPARK_URL = \"local[*]\"\n",
        "\n",
        "spark = SparkSession.builder.appName(APP_NAME) \\\n",
        "        .config('spark.ui.showConsoleProgress', 'false') \\\n",
        "        .getOrCreate()\n",
        "\n",
        "taxi = spark.read.load('terminal.csv',\n",
        "                       format='csv', header='true', inferSchema='true')\n",
        "\n",
        "taxi = taxi.fillna(0)\n",
        "\n",
        "taxi.registerTempTable(\"taxi\")\n",
        "\n",
        "print(spark.sql('SELECT hour, avg (pickups) from taxi group by hour order by  avg (pickups) desc').show(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk5QwZPRX3So",
        "outputId": "e7c0e9ed-cb99-4e99-b30e-fd7ae76abd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
            "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------------+\n",
            "|hour|      avg(pickups)|\n",
            "+----+------------------+\n",
            "|   8| 48.98208348725527|\n",
            "|   9| 45.74220335855324|\n",
            "|  18|45.131967515688444|\n",
            "|  19| 40.18456995201181|\n",
            "|  17| 37.68493909191584|\n",
            "|  12| 36.91678966789668|\n",
            "|  10|36.391031555637575|\n",
            "|  14|35.965867158671585|\n",
            "|   7| 35.93711855002774|\n",
            "|  13| 35.34939091915836|\n",
            "+----+------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}